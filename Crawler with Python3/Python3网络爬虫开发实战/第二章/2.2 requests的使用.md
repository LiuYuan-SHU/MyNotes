[TOC]

# 2.2 requests的使用

> `pip install requests`
> 

urllib库中的`urlopen`方法实际是以GET方式打开网页，而在`requests`中，其相应的方法就是`get`

而还有更多更加强大的方法：

- `post`
- `put`
- `delete`
- `patch`

这些功能，在requests库中，同样只是一个名字就能完成

## GET

[2.2.1_GET.py](https://github.com/LiuYuan-SHU/MyNotes/blob/master/Crawler%20with%20Python3/Python3%20web%20crawler%20development%20practice%EF%BC%88Edition2%EF%BC%89%20-%20Cui%20Qingcai/2_use_of_basic_libraries/2.2/2.2.1_get.py)

```python
def get(url: str | bytes,
        params: Any = ...,
        data: Any | None = ...,
        headers: Any | None = ...,
        cookies: Any | None = ...,
        files: Any | None = ...,
        auth: Any | None = ...,
        timeout: Any | None = ...,
        allow_redirects: bool = ...,
        proxies: Any | None = ...,
        hooks: Any | None = ...,
        stream: Any | None = ...,
        verify: Any | None = ...,
        cert: Any | None = ...,
        json: Any | None = ...) -> Response
```

网页的返回格式是JSON，所以如果想要直接解析返回结果，得到一个JSON格式的数据，可以直接调用`json`方法

不过需要注意的是，如果调对一个非JSON格式数据调用了这个方法，就会抛出`json.decoder.JSONDecodeError`异常

### 抓取网页——`get_web`

### 抓取二进制数据——`get_binary`

### 添加请求头——`add_headers`

## POST

[2.1.2_POST.py](https://github.com/LiuYuan-SHU/MyNotes/blob/master/Crawler%20with%20Python3/Python3%20web%20crawler%20development%20practice%EF%BC%88Edition2%EF%BC%89%20-%20Cui%20Qingcai/2_use_of_basic_libraries/2.2/2.1.2_post.py)

## 响应

```python
r = requests.get('https://ssr1.scrape.center/')
print(type(r.status)code, r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
```

requests库还提供了一个内置的状态码查询对象`requests.codes`，用法如下：

```python
r = requests.get('https://ssr1.scrapes.center')
exit() if not r.status_code == requests.codesok else print('Request Successfully')
```

## 高级用法

### 文件上传

[2.2.3_文件上传.py](https://github.com/LiuYuan-SHU/MyNotes/blob/7e9d23385ddb3577b91e17d453900a62a1cb75ed/Crawler%20with%20Python3/Python3%20web%20crawler%20development%20practice%EF%BC%88Edition2%EF%BC%89%20-%20Cui%20Qingcai/2_use_of_basic_libraries/2.2/2.2.3_%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6.py)

### Cookie设置

[2.2.4_Cookie设置.py](https://github.com/LiuYuan-SHU/MyNotes/blob/586a448366b9ca1556ad32ed5d87fdfbb6a59f56/Crawler%20with%20Python3/Python3%20web%20crawler%20development%20practice%EF%BC%88Edition2%EF%BC%89%20-%20Cui%20Qingcai/2_use_of_basic_libraries/2.2/2.2.4_Cookie%E8%AE%BE%E7%BD%AE.py)

我们可以通过`network -> github.com -> Headers -> Request Headers`中的`Cookie`文件来复制Cookie，然后用来进行登录（函数`login_with_cookie`）

当然，我们也可以对这个Cookie进行一些修改，重新封装然后提交（函数`request_cookie_jar`）

### Session维持

[2.2.5_Session维持.py](https://github.com/LiuYuan-SHU/MyNotes/blob/6104d54a0fa642b9be1558641e640a69827f0386/Crawler%20with%20Python3/Python3%20web%20crawler%20development%20practice%EF%BC%88Edition2%EF%BC%89%20-%20Cui%20Qingcai/2_use_of_basic_libraries/2.2/2.2.5_Session%E7%BB%B4%E6%8C%81.py)

直接利用`request`库中的`get`或`post`方法的确可以做到模拟网页的请求，但这两种方法相当于不同的Session，或者说用两个浏览器打开了不同的界面。

我们需要的是，第一次相当于打开浏览器，进入一个选项卡；第二次进入一个新的选项卡，而不是重新打开浏览器，同时我们也不想设置新的Cookie，这个时候就需要使用`Session对象`

利用Session可以做到模拟同一个会话而不用担心Cookie的问题，它通常在模拟登陆成功之后，进行下一步操作的时候用到

### SSL证书验证

[2.2.6_SSL证书验证.py](https://github.com/LiuYuan-SHU/MyNotes/blob/50b0be88e098e3bab4b3c814c2224aacc6caa4c8/Crawler%20with%20Python3/Python3%20web%20crawler%20development%20practice%EF%BC%88Edition2%EF%BC%89%20-%20Cui%20Qingcai/2_use_of_basic_libraries/2.2/2.2.6_SSL%E9%AA%8C%E8%AF%81.py)

现在很多的网站采用的都是HTTPS协议，但是有的网站可能没有，可能没有设置好，也有可能不被CA所认可，这个时候，当我们使用HTTPS去登录的时候就会出现SSL证书错误的提示。

如果我们不进行任何的修改，直接使用`request`库来请求网站，那么就会报错(`get_without_verify`)：

![](../../Python3网络爬虫开发实战.assets/SSL证书验证失败.png)

如果我们一定要爬取这样的网站，那么我们就需要使用`verify`参数控制是否验证证书。如果`verify`的值是`False`，那么在请求的时候就不再关心证书是否有效。（`get_with_verify`）

![](../../Python3网络爬虫开发实战.assets/使用Verify跳过验证.png)

通过这样的方法，我们发现我们跳过了验证。但是我们发现其中有一个警告：

```
InsecureRequestWarning: Unverified HTTPS request is being made to host 'ssr2.scrape.center'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  InsecureRequestWarning,
```

我们可以通过设置忽略警告的方式来屏蔽警告（`get_with_verify_disable_warning_method1()`）。
也可以通过将错误捕获到日志中的办法来忽略警告（`get_with_verify_disable_warning_method2()`）。

当然，我们也可以通过制定一个本地证书用作客户端证书，这可以是单个文件（包含秘钥和证书）或一个包含两个文件路径的元组：

```python
response = requests.get('https://ssr2.scrape.center', cert = '/path/server.crt', '/path/server.key')
print(response.status_code)
```

需要注意的是，本地私有证书的key必须是解密状态，加密状态的key是不支持的。

### 超时设置

为了防止服务器不能及时响应，应该设置一个超时时间，如果超过这个时间还没有得到相应，就报错。这需要用到`timeout`参数，其值是从发出请求到服务器返回相应的时间。

```python
r = requests.get('https://www.httpbin.org/get', timeout = 1)
print(r.status_code)
```

实际上，请求分为两个阶段：连接（connection）和读取（read）。通过上面的方法设置的是连接和读取的总时间。

当然，我们也可以分开设置：

```python
r = requests.get('https://www.httpbin.org/get', timeout = (5, 30))
```

如果我们想要永久等待，那么就可以将`timeout`设置为`None`，或者直接留空就好。

### 身份认证

在[2.1 urllib的使用](第二章/2.1%20urllib的使用.md#)的**验证**中我们提到了使用`urllib`来实现身份校验，但是实现起来相对繁琐。在`requests`库中，我们有更简单的方法：

```python
from requests.auth import HTTPBasicAuth

# 完整写法——使用HTTPBasicAuth构造方法
r = requests.get('https://ssr3.scrape.center', auth = HTTPBasicAuth('admin', 'admin'))
# 缩写——直接传入一个元组，自动调用HTTPBasicAuth构造方法
r = requests.get('https://ssr3.scrape.center', ('admin', 'admin'))

```

此外，我们还可以使用`OAuth`认证，不过需要安装`oauth`包、

使用`OAuth`认证的实例方法如下：

```python
import requests
from requests_oauthlib import OAuth1

url = 'https://api.twitter.com/1.1/account/verify_credentials.json'
auth = OAuth('YOUR_APP_KEY', 'YOUR_APP_SECRET',
			 'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')
requests.get(url, auth = auth)
```

### 代理设置

某些网站在测试的时候请求几次，都能正常获取内容。但是一旦开始大规模爬取，面对大规模且频繁的请求时，这些网站就可能弹出验证码，或者跳转到登录认证界面，更甚者可能会直接封禁客户端的IP，导致在一定时间内无法爬取。

那么，为了防止这样的情况的发生，我们需要设置代理来解决这个问题，这时就需要使用`proxies`参数。

```python
import requests  
  
proxies = {  
    'http': 'http://10.10.10.10:1080'  
    'https:' 'http://10.10.10.10:1080'}
```

当然，以上的代理只是一个示范，如果真的需要使用代理，可以到网络上去寻找提供代理的网站。

如果代理需要[上文](#身份认证)所提到的身份认证，可以使用类似`http://user:password@host:port`这样的语法来设置代理。

```python
proxies = {'https': 'http://user:password@host:port', }
requests.get('URL', proxies = proxies)
```

### Prepared Request

[2.2.7_Prepared Request.py](https://github.com/LiuYuan-SHU/MyNotes/blob/4a6f9295daaf7372104eeaefd04bbde9d9861a99/Crawler%20with%20Python3/Python3%20web%20crawler%20development%20practice%EF%BC%88Edition2%EF%BC%89%20-%20Cui%20Qingcai/2_use_of_basic_libraries/2.2/2.2.7_Prepared_Request.py)

我们所使用的的`get`、`post`等方法，在实际上，就是`requests`在发送请求的时候，在内部构造了一个`Request`对象，并给这个对象赋予了各种参数，包括`url`，`headers`，`data`等，然后直接把这个`Request`对象发送出去，请求成功后会再得到一个`Response`对象，解析这个对象即可。

那么`Request`对象的本质什么？其实就是`Prepared Request`。

我们用url，data和headers构造一个`Request`对象，然后使用`Session`类的`prepare_request`方法将其转换为一个`PreparedRequest`对象，然后调用`send`方法发送。

通过构造一个`Request`对象，我们可以将请求独立地当做对象来看待，在一些场景中我们就可以直接操作这个对象，更灵活地实现各种调度和操作。